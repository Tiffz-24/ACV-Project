{"cells":[{"cell_type":"code","execution_count":181,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":108008,"status":"ok","timestamp":1715185069761,"user":{"displayName":"Costanza Siniscalchi","userId":"01416823643866606228"},"user_tz":240},"id":"c1E7uVHQ9fn0","outputId":"4542837a-7f50-4ec1-adea-8354ef681eda"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (2.2.0)\n","Requirement already satisfied: torchvision in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (0.17.0)\n","Requirement already satisfied: filelock in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (4.8.0)\n","Requirement already satisfied: sympy in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (2023.12.2)\n","Requirement already satisfied: numpy in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torchvision) (1.26.2)\n","Requirement already satisfied: requests in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torchvision) (10.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: torch in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (2.2.0)\n","Requirement already satisfied: torchvision in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (0.17.0)\n","Requirement already satisfied: pillow in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (10.1.0)\n","Requirement already satisfied: filelock in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (4.8.0)\n","Requirement already satisfied: sympy in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (2023.12.2)\n","Requirement already satisfied: numpy in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torchvision) (1.26.2)\n","Requirement already satisfied: requests in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: torch in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (2.2.0)\n","Requirement already satisfied: torchvision in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (0.17.0)\n","Requirement already satisfied: matplotlib in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (3.8.2)\n","Requirement already satisfied: filelock in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (4.8.0)\n","Requirement already satisfied: sympy in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (2023.12.2)\n","Requirement already satisfied: numpy in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torchvision) (1.26.2)\n","Requirement already satisfied: requests in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torchvision) (10.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from matplotlib) (4.44.3)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: tqdm in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (4.65.0)\n"]}],"source":["!pip install torch torchvision\n","!pip install torch torchvision pillow\n","!pip install torch torchvision matplotlib\n","!pip install tqdm\n"]},{"cell_type":"code","execution_count":182,"metadata":{"executionInfo":{"elapsed":8282,"status":"ok","timestamp":1715185078033,"user":{"displayName":"Costanza Siniscalchi","userId":"01416823643866606228"},"user_tz":240},"id":"DiuT36Lp8AQU"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torch.optim import Adam\n","from tqdm.notebook import tqdm\n","from torch import optim\n"]},{"cell_type":"code","execution_count":183,"metadata":{"id":"kLlQNWTk-fjN"},"outputs":[],"source":["pcam_directory =  '/Users/costanzasiniscalchi/Documents/Senior/ACV/project/histopathologic-cancer-detection'"]},{"cell_type":"code","execution_count":184,"metadata":{"id":"Ncz512lk9Aqk"},"outputs":[],"source":["class CGanGenerator(nn.Module):\n","    def __init__(self, n_classes=2, output_channels=3):\n","        super().__init__()\n","        self.n_classes = n_classes\n","        self.output_channels = output_channels\n","        self.embedding = nn.Embedding(n_classes, 96 * 96)  # Adjusted embedding dimension\n","        self.conv1 = nn.Conv2d(3, 1024, kernel_size=3, stride=1, padding=1)\n","        self.conv_blocks = nn.ModuleList([\n","            self._get_conv_transpose_block(1024 + 1, 512),  # Adjusted for one additional channel from embedding\n","            self._get_conv_transpose_block(512, 256),\n","            self._get_conv_transpose_block(256, 128),\n","            self._get_conv_transpose_block(128, output_channels, last_block=True)\n","        ])\n","\n","    def _get_conv_transpose_block(self, in_channels, out_channels, last_block=False):\n","        layers = [\n","            nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(True)\n","        ]\n","        if last_block:\n","            layers[-1] = nn.Tanh()\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, labels):\n","        labels = labels.long()\n","        x = F.relu(self.conv1(x))\n","        y = self.embedding(labels).view(-1, 1, 96, 96)  # Adjusted view to match the new embedding dimension\n","        x = torch.cat([x, y], 1)  # Concatenate label embedding to feature map\n","        for block in self.conv_blocks:\n","            x = block(x)\n","        return x\n"]},{"cell_type":"code","execution_count":185,"metadata":{"id":"gNRzcunQ9C2P"},"outputs":[],"source":["class CGanDiscriminator(nn.Module):\n","    def __init__(self, input_shape=(3, 96, 96), n_classes=2):\n","        super().__init__()\n","        self.n_classes = n_classes\n","        self.conv_blocks = nn.ModuleList([\n","            self._get_conv_block(3, 64, first_block=True),\n","            self._get_conv_block(64, 128),\n","            self._get_conv_block(128, 256),\n","            self._get_conv_block(256, 512),\n","            self._get_conv_block(512, 512)\n","        ])\n","        # Calculate the output size of the last conv layer dynamically\n","        example_input = torch.rand(1, *input_shape)\n","        output_size = self.forward_features(example_input).view(-1).shape[0]\n","        print(\"Output size for linear layer:\", output_size)  # Debug print\n","\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(output_size, 1),\n","            nn.Sigmoid()\n","        )\n","        self.class_label = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(output_size, n_classes),\n","            nn.Softmax(dim=1)\n","        )\n","\n","    def _get_conv_block(self, in_channels, out_channels, first_block=False):\n","        layers = [\n","            nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1),\n","            nn.LeakyReLU(0.2, True),\n","            nn.Dropout(0.5)\n","        ]\n","        if not first_block:\n","            layers.insert(1, nn.BatchNorm2d(out_channels))\n","        return nn.Sequential(*layers)\n","\n","    def forward_features(self, x):\n","        for block in self.conv_blocks:\n","            x = block(x)\n","        return x\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        real_fake = self.classifier(x)\n","        class_label = self.class_label(x)\n","        return real_fake, class_label\n"]},{"cell_type":"code","execution_count":186,"metadata":{"id":"m4xwrY-n9FiU"},"outputs":[],"source":["class CGan(nn.Module):\n","    def __init__(self, generator, discriminator):\n","        super().__init__()\n","        self.generator = generator\n","        self.discriminator = discriminator\n","\n","    def forward(self, noise, labels):\n","        self.discriminator.eval()\n","        gen_imgs = self.generator(noise, labels)\n","        validity, _ = self.discriminator(gen_imgs)\n","        return validity\n","\n"]},{"cell_type":"code","execution_count":187,"metadata":{"id":"dYEx-1Jg9k1Y"},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from PIL import Image\n","\n","\n","def get_transforms():\n","    return transforms.Compose([\n","        transforms.Resize((96, 96)),  # Assuming images need to be resized\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","    ])\n","\n","def generate_real_samples(batch, device='cpu'):\n","    \"\"\"Prepares a batch of real samples from the DataLoader\"\"\"\n","    images, latent_vectors, labels = batch  # Adjust this line based on your DataLoader output\n","    images = images.to(device)\n","    labels = labels.to(device)\n","    y = torch.ones(images.size(0), 1, dtype=torch.float32).to(device)  # Labels for real samples are all ones\n","    return images, labels, y\n","\n","def generate_latent_points(latent_dim, n_samples, n_classes=2, device='cpu'):\n","    \"\"\"Generates a batch of latent vectors of random points\"\"\"\n","    z_input = torch.randn(n_samples, latent_dim, device=device)\n","    labels = torch.randint(0, n_classes, (n_samples,), device=device)\n","    return z_input, labels\n","\n","def generate_fake_samples(generator, latent_dim, n_samples, device='cpu'):\n","    \"\"\"Generates a batch of fake samples from latent vectors using the generator model.\"\"\"\n","    z_input, labels = generate_latent_points(latent_dim, n_samples, device=device)\n","    with torch.no_grad():\n","        # Make sure to pass both z_input and labels to the generator\n","        images = generator(z_input, labels)\n","    y = torch.zeros(n_samples, 1, dtype=torch.float32).to(device)  # Labels for fake samples are all zeros\n","    return images, labels, y\n"]},{"cell_type":"code","execution_count":188,"metadata":{"id":"UrJwgvx0-GK2"},"outputs":[],"source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from math import sqrt\n","from os.path import join\n","from dataclasses import dataclass\n","\n","@dataclass\n","class TrainParam:\n","    n_epochs: int\n","    batch_size: int\n","    latent_dim: int\n","    epoch_checkpoint: int\n","    n_summary_samples: int\n","    starting_epoch: int = 0\n","    output_path: str = './'  # Default output path\n","    model_path: str = './'   # Default model path\n","\n","import torch\n","from torch import optim\n","import os\n","\n","def trainer(gan, train_loader, train_param, device):\n","    generator, discriminator = gan.generator, gan.discriminator\n","    \n","\n","    # Establish criterion for loss calculation\n","    adversarial_loss = torch.nn.BCELoss()\n","    reconstruction_loss = torch.nn.L1Loss()\n","\n","    # Optimizers\n","    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","    generator.to(device)\n","    discriminator.to(device)\n","\n","    for epoch in range(train_param.starting_epoch, train_param.n_epochs):\n","        epoch_loss_D, epoch_loss_G = 0, 0\n","        train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{train_param.n_epochs}', leave=False)\n","        for i, (blurred_images, real_images, labels) in enumerate(train_loader):\n","            batch_size = blurred_images.size(0)\n","            real_labels = torch.ones(batch_size, 1, device=device)\n","            fake_labels = torch.zeros(batch_size, 1, device=device)\n","\n","            # Transfer data to device\n","            real_images = real_images.to(device)\n","            blurred_images = blurred_images.to(device)\n","            labels = labels.to(device)\n","\n","            # -----------------\n","            # Train Discriminator\n","            # -----------------\n","            optimizer_D.zero_grad()\n","\n","            # Real samples\n","            real_preds, _ = discriminator(real_images)\n","            d_real_loss = adversarial_loss(real_preds, real_labels)\n","\n","            # Fake samples\n","            generated_images = generator(blurred_images, labels)\n","            fake_preds, _ = discriminator(generated_images.detach())\n","            d_fake_loss = adversarial_loss(fake_preds, fake_labels)\n","\n","            # Total discriminator loss\n","            d_loss = (d_real_loss + d_fake_loss) / 2\n","            d_loss.backward()\n","            optimizer_D.step()\n","\n","            # -----------------\n","            # Train Generator\n","            # -----------------\n","            optimizer_G.zero_grad()\n","\n","            # Another forward pass for the generated images\n","            fake_preds, _ = discriminator(generated_images)\n","            g_adv_loss = adversarial_loss(fake_preds, real_labels)\n","            g_rec_loss = reconstruction_loss(generated_images, real_images)\n","\n","            # Total generator loss\n","            g_loss = 0.001 * g_adv_loss + g_rec_loss\n","            g_loss.backward()\n","            optimizer_G.step()\n","\n","            # Update progress bar\n","            train_bar.set_postfix({\n","                'D Loss': f'{d_loss.item():.4f}',\n","                'G Loss': f'{g_loss.item():.4f}'\n","            })\n","\n","            epoch_loss_D += d_loss.item()\n","            epoch_loss_G += g_loss.item()\n","\n","            if (i + 1) % 100 == 0:\n","                print(f\"Epoch {epoch+1}/{train_param.n_epochs}, Batch {i+1}/{len(train_loader)}, Discriminator Loss: {d_loss.item()}, Generator Loss: {g_loss.item()}\")\n","\n","        # Save models periodically\n","        if (epoch + 1) % train_param.epoch_checkpoint == 0:\n","            torch.save(generator.state_dict(), os.path.join(train_param.model_path, f'generator_epoch_{epoch+1}.pth'))\n","            torch.save(discriminator.state_dict(), os.path.join(train_param.model_path, f'discriminator_epoch_{epoch+1}.pth'))\n","\n","\n","\n","\n","\n","def plot_images(X, figsize, n_samples, epoch, output_path=None):\n","    \"\"\"Plot and save generated images.\"\"\"\n","    plt.figure(figsize=figsize)\n","    sample_sqrt = int(sqrt(n_samples))\n","    plt.subplots_adjust(right=0.9, left=0.0, top=0.9, bottom=0.0, hspace=0.02, wspace=0.02)\n","    for i in range(n_samples):\n","        plt.subplot(sample_sqrt, sample_sqrt, 1 + i)\n","        plt.axis('off')\n","        plt.imshow(X[i].transpose(1, 2, 0))  # Transpose as needed depending on data format\n","    plt.show()\n","    if output_path:\n","        plt.savefig(join(output_path, f'generated_plot_{epoch}.png'))\n","        plt.close()\n"]},{"cell_type":"code","execution_count":189,"metadata":{"id":"3F57zX9K4aN3"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import os\n","import random\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","\n","class BlurredPCamDataset(Dataset):\n","    def __init__(self, root_dir, label_mapping, transform=None):\n","        self.root_dir = root_dir\n","        self.label_mapping = label_mapping\n","        self.transform = transform\n","        self.all_fps = [os.path.join(root_dir, fp) for fp in label_mapping.keys()]\n","        self.kernel_cache = {}  # Cache Gaussian kernels\n","\n","    def __getitem__(self, index):\n","        image_fp = self.all_fps[index]\n","        slide_id = os.path.basename(image_fp)\n","        image = Image.open(image_fp).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # Convert image to tensor (if not already done by transform)\n","        if not isinstance(image, torch.Tensor):\n","            image = transforms.ToTensor()(image)\n","\n","        # Degrade the image by adding blur and noise\n","        degraded_image, _ = self.degrade_image(image)\n","\n","        label = self.label_mapping[slide_id]\n","\n","        return degraded_image, image, label\n","\n","    def __len__(self):\n","        return len(self.all_fps)\n","\n","    def create_gaussian_kernel(self, size, sigma):\n","        if (size, sigma) not in self.kernel_cache:\n","            ax = torch.linspace(-(size - 1) / 2., (size - 1) / 2., size)\n","            xx, yy = torch.meshgrid(ax, ax)\n","            kernel = torch.exp(-(xx**2 + yy**2) / (2 * sigma**2))\n","            kernel = kernel / torch.sum(kernel)\n","            self.kernel_cache[(size, sigma)] = kernel.view(1, 1, size, size)\n","        return self.kernel_cache[(size, sigma)]\n","\n","    def degrade_image(self, image):\n","        c, h, w = image.shape\n","        patch_size = random.randint(10, 32)  # Random patch size between 10 and 32\n","        x = random.randint(0, w - patch_size)\n","        y = random.randint(0, h - patch_size)\n","\n","        # Extract patch\n","        patch = image[:, y:y + patch_size, x:x + patch_size]\n","\n","        # Apply Gaussian blur\n","        if random.random() > 0.5:\n","            size = random.choice([3, 5, 7])  # Kernel size choice\n","            sigma = random.uniform(0.5, 1.5)  # Sigma choice\n","            blur_kernel = self.create_gaussian_kernel(size, sigma).repeat(c, 1, 1, 1).to(image.device)\n","            pad_size = size // 2\n","            padded_patch = F.pad(patch, (pad_size, pad_size, pad_size, pad_size), mode='reflect')\n","            blurred_patch = F.conv2d(padded_patch.unsqueeze(0), blur_kernel, padding=0, stride=1, groups=c).squeeze(0)\n","            # Ensure the size matches the original patch\n","            blurred_patch = blurred_patch[:, pad_size:-pad_size, pad_size:-pad_size]\n","            if blurred_patch.shape[1] != patch_size or blurred_patch.shape[2] != patch_size:\n","                blurred_patch = F.interpolate(blurred_patch.unsqueeze(0), size=(patch_size, patch_size), mode='bilinear').squeeze(0)\n","            image[:, y:y + patch_size, x:x + patch_size] = blurred_patch\n","\n","        # Optionally add Gaussian noise\n","        if random.random() > 0.5:\n","            noise = torch.randn_like(patch) * 0.05  # Noise addition\n","            image[:, y:y + patch_size, x:x + patch_size] += noise\n","\n","        return image, (x, y, patch_size, patch_size)\n","\n","    def show_dataset_images(self, indices, ncols=3):\n","        plt.figure(figsize=(15, 5))\n","        for i, idx in enumerate(indices):\n","            degraded_image, original_image, _ = self[idx]\n","            plt.subplot(1, ncols, i + 1)\n","            plt.imshow(degraded_image.permute(1, 2, 0).numpy())\n","            plt.title(f\"Index: {idx}\")\n","            plt.axis('off')\n","        plt.show()\n"]},{"cell_type":"code","execution_count":190,"metadata":{"id":"xkZYT7ro_KQ4"},"outputs":[],"source":["import csv\n","label_mapping = {}\n","with open(os.path.join(pcam_directory, 'train_labels.csv'), 'r') as f:\n","    reader = csv.reader(f)\n","    next(reader)  # To skip the header\n","    label_mapping = {slide_id +'.tif': int(label) for [slide_id, label] in reader}\n","all_fps = [fp for fp in os.listdir(os.path.join(pcam_directory, 'train'))]\n","for fp in all_fps: assert fp[-4:] == '.tif', fp[-4:]\n"]},{"cell_type":"code","execution_count":191,"metadata":{"id":"cV2ekVmZ-tNi"},"outputs":[],"source":["\n","train_transforms = transforms.Compose([\n","    transforms.ColorJitter(),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomVerticalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Imagenet statistics\n","])\n","val_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Imagenet statistics\n","])\n","test_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Imagenet statistics\n","])\n"]},{"cell_type":"code","execution_count":192,"metadata":{"id":"02Y7hEedA-fZ"},"outputs":[],"source":["permutation = np.random.permutation(len(all_fps))\n","num_train = int(len(permutation) * 0.6)\n","\n","train_dataset = BlurredPCamDataset(os.path.join(pcam_directory, 'train'), {fp: label_mapping[fp] for fp in all_fps[:num_train] if fp in label_mapping}, transform=train_transforms)\n","test_dataset = BlurredPCamDataset(os.path.join(pcam_directory, 'train'), {fp: label_mapping[fp] for fp in all_fps[num_train:] if fp in label_mapping}, transform=test_transforms)\n","\n"]},{"cell_type":"code","execution_count":193,"metadata":{"id":"TeKVbaXKBCOM"},"outputs":[],"source":["\n","train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)"]},{"cell_type":"code","execution_count":194,"metadata":{"id":"cHb_kVr2uaQz"},"outputs":[],"source":["import torchsummary"]},{"cell_type":"code","execution_count":195,"metadata":{"id":"14PNOrkCuZAe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Output size for linear layer: 4608\n","Generator Summary:\n","\n","Discriminator Summary:\n"]}],"source":["latent_dimension = 100  # Dimension of the latent space\n","n_classes = 2          # Number of classes (for conditional GAN)\n","output_channels = 3    # Output channels for the generated images (typically 3 for RGB images)\n","input_shape = (3, 96, 96)  # Input shape for the discriminator (channels, height, width)\n","\n","train_param = TrainParam(\n","    n_epochs=2000,\n","    batch_size=128,\n","    latent_dim=100,\n","    epoch_checkpoint=20,\n","    n_summary_samples=36\n",")\n","\n","# generator = CGanGenerator(latent_dimension=latent_dimension, n_classes=n_classes, output_channels=output_channels)\n","\n","generator = CGanGenerator(n_classes=n_classes, output_channels=output_channels)\n","\n","discriminator = CGanDiscriminator(input_shape=input_shape, n_classes=n_classes)\n","\n","# Configure device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Move models to the correct device\n","generator.to(device)\n","discriminator.to(device)\n","\n","# Printing summaries\n","# The generator expects a latent space vector and class labels as input\n","# The discriminator expects an image of shape input_shape\n","\n","print(\"Generator Summary:\")\n","image_size = (3, 96, 96)\n","# torchsummary.summary(generator, input_size=image_size, device=str(device))\n","\n","print(\"\\nDiscriminator Summary:\")\n","# torchsummary.summary(discriminator, input_shape, device=str(device))\n","gan = CGan(generator=generator, discriminator=discriminator)\n"]},{"cell_type":"code","execution_count":196,"metadata":{"id":"OaL4LL_tPE-_"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":197,"metadata":{"id":"uZKi6Udqh9Ji"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8fa2c8294d8d4db8b828cc14d189cbf6","version_major":2,"version_minor":0},"text/plain":["Epoch 1/2000:   0%|          | 0/1032 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["gan_model, history = trainer(gan, train_dataloader, train_param, device=device)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
