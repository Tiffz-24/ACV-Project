{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "\n",
        "def train_network(net, trainloader, criterion=CrossEntropyLoss(), optimizer_class=Adam, n_epochs=50):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device: \", device)\n",
        "\n",
        "    net.to(device)\n",
        "\n",
        "    # Instantiate the criterion if it's a class.\n",
        "    if isinstance(criterion, type):\n",
        "        criterion = criterion()\n",
        "\n",
        "    # Instantiate the optimizer with the network parameters.\n",
        "    optimizer = optimizer_class(net.parameters())\n",
        "\n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "        print(\"starting epoch \", epoch)\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            print(\"got data\")\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            print(\"inputs and labels to device\")\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            print(\"zero grad\")\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # print statistics\n",
        "            if i % 1000 == 0 and i > 0:\n",
        "                print(f'Epoch={epoch + 1} Iter={i + 1:5d} Loss={running_loss / 1000:.3f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "    return net"
      ],
      "metadata": {
        "id": "Fxs-DIGoT4Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.skip_connection = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.skip_connection = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"RESNET BLOCK FORWARD: \", x.shape)\n",
        "        identity = self.skip_connection(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += identity\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, input_channels=3, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Define ResNet layers\n",
        "        self.layer1 = ResNetBlock(64, 64)\n",
        "        self.layer2 = ResNetBlock(64, 128, stride=2)\n",
        "        self.layer3 = ResNetBlock(128, 256, stride=2)\n",
        "        self.layer4 = ResNetBlock(256, 512, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # This makes the network input size agnostic\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"FORWARD: \", x.shape)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)  # Adaptive pooling\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "hnpyVgp6T46z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "class HEDataset(Dataset):\n",
        "    def __init__(self, images_file_path, labels_file_path, transform=None, crop = None):\n",
        "        self.images_file_path = images_file_path\n",
        "        self.labels_file_path = labels_file_path\n",
        "        self.transform = transform\n",
        "        self.crop = crop\n",
        "\n",
        "        # Load the file to check the shape of the dataset\n",
        "        self._print_dataset_shapes()\n",
        "\n",
        "    def _print_dataset_shapes(self):\n",
        "        with h5py.File(self.images_file_path, 'r') as images_file:\n",
        "            # Assuming you want to know the shape of the first image dataset\n",
        "\n",
        "            first_image_key = list(images_file.keys())[0]\n",
        "            image_shape = images_file[first_image_key].shape\n",
        "            self.length = image_shape[0]\n",
        "            if self.crop:\n",
        "                print(f\"Image shape {image_shape} will crop to \", [self.length, 3, self.crop, self.crop])\n",
        "            else:\n",
        "                print(f\"Image shape {[self.length, 3, image_shape[2], image_shape[3]]}\")\n",
        "        with h5py.File(self.labels_file_path, 'r') as labels_file:\n",
        "            # Assuming you want to know the shape of the first label dataset\n",
        "            first_label_key = list(labels_file.keys())[0]\n",
        "            label_shape = labels_file[first_label_key].shape\n",
        "\n",
        "            if label_shape[0] != self.length:\n",
        "                print(\"BAD: make error. Dataset x and y sizes do not match\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        with h5py.File(self.images_file_path, 'r') as images_file:\n",
        "            first_image_key = list(images_file.keys())[0]\n",
        "            image_data = list(images_file[first_image_key])[idx]\n",
        "            if image_data.shape[-1] == 3:\n",
        "                image_data = image_data.transpose((2, 0, 1))  # RGB filter in 0 position\n",
        "\n",
        "        with h5py.File(self.labels_file_path, 'r') as labels_file:\n",
        "            first_label_key = list(labels_file.keys())[0]\n",
        "            label_data = list(labels_file[first_label_key])[idx]\n",
        "        if self.crop:\n",
        "            w, h = image_data.shape[1], image_data.shape[2]\n",
        "            startx = w // 2 - self.crop // 2\n",
        "            starty = h // 2 - self.crop // 2\n",
        "            image_data = image_data[:, starty:starty + self.crop, startx:startx + self.crop]\n",
        "\n",
        "        # Convert to PIL image using PIL Image module\n",
        "        image_data = Image.fromarray(np.uint8(image_data.transpose(1, 2, 0)))\n",
        "\n",
        "        if self.transform:\n",
        "            image_data = self.transform(image_data)\n",
        "\n",
        "        return image_data, torch.tensor(label_data, dtype=torch.float32)\n",
        "\n",
        "def load_datasets(test_x,test_y, train_x, train_y, valid_x,valid_y, transform=None, crop = None):\n",
        "\n",
        "    test_dataset = HEDataset(test_x, test_y, transform, crop)\n",
        "    train_dataset = HEDataset(train_x, train_y, transform, crop)\n",
        "    validation_dataset = HEDataset(valid_x, valid_y, transform, crop)\n",
        "\n",
        "    return test_dataset, train_dataset, validation_dataset\n",
        "\n",
        "def load_dataloaders(test_dataset, train_dataset, validation_dataset, batch_size = 64, shuffle = True, num_workers = 4):\n",
        "\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "\n",
        "    return test_dataloader, train_dataloader, validation_dataloader"
      ],
      "metadata": {
        "id": "p2af4XKtT7Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/1krtqpLh9Wi9QijMU7EcMOmlOB6ezwPI4/view?usp=sharing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19KCsMXY8RWa",
        "outputId": "b2cca54f-c5b6-4bae-a6f4-a223fdf7dcae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1krtqpLh9Wi9QijMU7EcMOmlOB6ezwPI4\n",
            "From (redirected): https://drive.google.com/uc?id=1krtqpLh9Wi9QijMU7EcMOmlOB6ezwPI4&confirm=t&uuid=155d21ea-f2f1-432c-a664-c70869fcb813\n",
            "To: /content/pcam.zip\n",
            "100% 8.02G/8.02G [01:53<00:00, 70.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip pcam.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO9uHd12CuIv",
        "outputId": "13c95ce2-f246-4ec4-aa1b-9db1245872b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  pcam.zip\n",
            "   creating: pcam/\n",
            "  inflating: pcam/camelyonpatch_level_2_split_train_meta.csv  \n",
            "  inflating: __MACOSX/pcam/._camelyonpatch_level_2_split_train_meta.csv  \n",
            "  inflating: pcam/camelyonpatch_level_2_split_test_y.h5.gz  \n",
            "  inflating: __MACOSX/pcam/._camelyonpatch_level_2_split_test_y.h5.gz  \n",
            "  inflating: pcam/camelyonpatch_level_2_split_train_y.h5.gz  \n",
            "  inflating: __MACOSX/pcam/._camelyonpatch_level_2_split_train_y.h5.gz  \n",
            "  inflating: pcam/camelyonpatch_level_2_split_test_x.h5.gz  \n",
            "  inflating: __MACOSX/pcam/._camelyonpatch_level_2_split_test_x.h5.gz  \n",
            "  inflating: pcam/camelyonpatch_level_2_split_test_meta.csv  \n",
            "  inflating: __MACOSX/pcam/._camelyonpatch_level_2_split_test_meta.csv  \n",
            "  inflating: pcam/camelyonpatch_level_2_split_train_x.h5.zip  \n",
            "  inflating: __MACOSX/pcam/._camelyonpatch_level_2_split_train_x.h5.zip  \n",
            "  inflating: pcam/camelyonpatch_level_2_split_valid_x.h5.gz  \n",
            "  inflating: __MACOSX/pcam/._camelyonpatch_level_2_split_valid_x.h5.gz  \n",
            "  inflating: pcam/camelyonpatch_level_2_split_train_mask.h5.gz  \n",
            "  inflating: __MACOSX/pcam/._camelyonpatch_level_2_split_train_mask.h5.gz  \n",
            "  inflating: pcam/camelyonpatch_level_2_split_valid_meta.csv  \n",
            "  inflating: __MACOSX/pcam/._camelyonpatch_level_2_split_valid_meta.csv  \n",
            "  inflating: pcam/camelyonpatch_level_2_split_valid_y.h5.gz  \n",
            "  inflating: __MACOSX/pcam/._camelyonpatch_level_2_split_valid_y.h5.gz  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/pcam/camelyonpatch_level_2_split_train_x.h5.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_xX6dcoLcRX",
        "outputId": "85f07e80-4eba-480c-e90b-a51e5aa277b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/pcam/camelyonpatch_level_2_split_train_x.h5.zip\n",
            "  inflating: camelyonpatch_level_2_split_train_x.h5  \n",
            "  inflating: __MACOSX/._camelyonpatch_level_2_split_train_x.h5  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip /content/pcam/camelyonpatch_level_2_split_train_y.h5.gz\n",
        "!gunzip /content/pcam/camelyonpatch_level_2_split_test_y.h5.gz\n",
        "!gunzip /content/pcam/camelyonpatch_level_2_split_test_x.h5.gz\n",
        "!gunzip /content/pcam/camelyonpatch_level_2_split_valid_x.h5.gz\n",
        "!gunzip /content/pcam/camelyonpatch_level_2_split_valid_y.h5.gz"
      ],
      "metadata": {
        "id": "nYbj0cGIMMKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-TvaPVjPiv_",
        "outputId": "47607f3f-947d-4c77-8f20-f0a8ac848d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn7NetWiTJu6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "base = '/content/pcam/'\n",
        "test_x = base+'camelyonpatch_level_2_split_test_x.h5'\n",
        "test_y = base+'camelyonpatch_level_2_split_test_y.h5'\n",
        "train_mask = base+'camelyonpatch_level_2_split_train_mask.h5'\n",
        "train_x = 'camelyonpatch_level_2_split_train_x.h5'\n",
        "train_y = base+'camelyonpatch_level_2_split_train_y.h5'\n",
        "valid_x = base+'camelyonpatch_level_2_split_valid_x.h5'\n",
        "valid_y = base+'camelyonpatch_level_2_split_valid_y.h5'\n",
        "\n",
        "#Load in metadata\n",
        "train_csv = base+'camelyonpatch_level_2_split_train_meta.csv'\n",
        "test_csv = base+'camelyonpatch_level_2_split_test_meta.csv'\n",
        "valid_csv = base+'camelyonpatch_level_2_split_valid_meta.csv'\n",
        "\n",
        "train_meta = pd.read_csv(train_csv)\n",
        "test_meta = pd.read_csv(test_csv)\n",
        "val_meta = pd.read_csv(valid_csv)\n",
        "\n",
        "def convert_to_int(df):\n",
        "    df[\"tumor_patch\"] = df[\"tumor_patch\"].astype(int)\n",
        "    df[\"center_tumor_patch\"] = df[\"center_tumor_patch\"].astype(int)\n",
        "    return df\n",
        "\n",
        "train_meta = convert_to_int(train_meta)\n",
        "test_meta = convert_to_int(test_meta)\n",
        "val_meta = convert_to_int(val_meta)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts to tensor and preserves channels\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize each channel of RGB\n",
        "])\n",
        "\n",
        "test_dataset, train_dataset, validation_dataset = load_datasets(test_x,test_y, train_x, train_y, valid_x,valid_y, transform= transform, crop=32)\n",
        "test_dataloader, train_dataloader, validation_dataloader = load_dataloaders(test_dataset, train_dataset, validation_dataset, batch_size = 128, shuffle = True)\n",
        "\n",
        "model = ResNet()\n",
        "\n",
        "\n",
        "net = train_network(model, train_dataloader) #using default CrossEntropyLoss and Adam Optimizer with 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import h5py\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "class HEDataset(Dataset):\n",
        "    def __init__(self, images_file_path, labels_file_path, transform=None, crop=None):\n",
        "        self.images_file_path = images_file_path\n",
        "        self.labels_file_path = labels_file_path\n",
        "        self.transform = transform\n",
        "        self.crop = crop\n",
        "        self.images, self.labels = self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        images, labels = [], []\n",
        "        with h5py.File(self.images_file_path, 'r') as images_file:\n",
        "            first_image_key = list(images_file.keys())[0]\n",
        "            images = np.array(images_file[first_image_key])\n",
        "        with h5py.File(self.labels_file_path, 'r') as labels_file:\n",
        "            first_label_key = list(labels_file.keys())[0]\n",
        "            labels = np.array(labels_file[first_label_key])\n",
        "        return images, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_data = self.images[idx]\n",
        "        label_data = self.labels[idx]\n",
        "        if self.crop:\n",
        "            image_data = self.crop_image(image_data)\n",
        "        image_data = Image.fromarray(image_data)\n",
        "        if self.transform:\n",
        "            image_data = self.transform(image_data)\n",
        "        return image_data, torch.tensor(label_data, dtype=torch.float32)\n",
        "\n",
        "    def crop_image(self, image_data):\n",
        "        w, h = image_data.shape[1], image_data.shape[0]\n",
        "        startx = w // 2 - self.crop // 2\n",
        "        starty = h // 2 - self.crop // 2\n",
        "        return image_data[starty:starty + self.crop, startx:startx + self.crop]\n",
        "\n",
        "    def update_exclusion_list(self, exclude_indices):\n",
        "        \"\"\" Update the list of indices to exclude from the dataset. \"\"\"\n",
        "        self.exclude_indices = set(exclude_indices)\n",
        "\n",
        "    def degrade_all_images(self):\n",
        "        return [self.degrade_image(image) for image in self.images]\n",
        "\n",
        "    def degrade_image(self, image, focus_area_size=32, sigma=20):\n",
        "        x = random.randint(0, image.shape[0] - focus_area_size)\n",
        "        y = random.randint(0, image.shape[1] - focus_area_size)\n",
        "        patch = image[x:x + focus_area_size, y:y + focus_area_size]\n",
        "        blurred_patch = cv2.GaussianBlur(patch, (5, 5), sigma)\n",
        "        noise = np.random.normal(0, 10, blurred_patch.shape)\n",
        "        degraded_patch = blurred_patch + noise\n",
        "        image[x:x + focus_area_size, y:y + focus_area_size] = degraded_patch.astype(np.uint8)\n",
        "        return image\n",
        "\n",
        "def show_dataset_images(dataset, indices, ncols=3):\n",
        "    plt.figure(figsize=(15, 5))  # Adjust the size as needed\n",
        "    for i, idx in enumerate(indices):\n",
        "        image, _ = dataset[idx]\n",
        "        if isinstance(image, torch.Tensor):  # Check if the image needs to be converted from a tensor\n",
        "            image = image.permute(1, 2, 0).numpy()  # Adjust dimensions for Matplotlib\n",
        "        plt.subplot(1, ncols, i + 1)\n",
        "        plt.imshow(image)\n",
        "        plt.title(f\"Index: {idx}\")\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "dataset = HEDataset('camelyonpatch_level_2_split_train_x.h5', '/content/pcam/camelyonpatch_level_2_split_train_y.h5', transform=None, crop=128)\n",
        "degraded_images = dataset.degrade_all_images()\n",
        "show_dataset_images(degraded_images, range(12), ncols=3)\n"
      ],
      "metadata": {
        "id": "5JBJgMk336F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code below based off of slideflow implementation of DeepFocus algorithm (https://github.com/jamesdolezal/slideflow/blob/master/slideflow/slide/qc/deepfocus.py)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DeepFocusV3(nn.Module):\n",
        "    def __init__(self, filters=(32, 32, 64, 128, 128), kernel_sizes=(5, 3, 3, 3, 3), fc=(128, 64)):\n",
        "        super(DeepFocusV3, self).__init__()\n",
        "        self.filters = filters\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "\n",
        "        # Assuming the input images are 64x64 RGB images\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, filters[0], kernel_size=kernel_sizes[0], padding='same')\n",
        "        self.bn1 = nn.BatchNorm2d(filters[0])\n",
        "\n",
        "        self.conv2 = nn.Conv2d(filters[0], filters[1], kernel_size=kernel_sizes[1], padding='same')\n",
        "        self.bn2 = nn.BatchNorm2d(filters[1])\n",
        "\n",
        "        self.conv3 = nn.Conv2d(filters[1], filters[2], kernel_size=kernel_sizes[2], padding='same')\n",
        "        self.bn3 = nn.BatchNorm2d(filters[2])\n",
        "        self.pool1 = nn.MaxPool2d(2, padding='same')\n",
        "\n",
        "        self.conv4 = nn.Conv2d(filters[2], filters[3], kernel_size=kernel_sizes[3], padding='same')\n",
        "        self.bn4 = nn.BatchNorm2d(filters[3])\n",
        "        self.pool2 = nn.MaxPool2d(2, padding='same')\n",
        "\n",
        "        self.conv5 = nn.Conv2d(filters[3], filters[4], kernel_size=kernel_sizes[4], padding='same')\n",
        "        self.bn5 = nn.BatchNorm2d(filters[4])\n",
        "        self.pool3 = nn.MaxPool2d(2, padding='same')\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(filters[4] * 8 * 8, fc[0])  # Adjust the sizing calculation as necessary\n",
        "        self.bn6 = nn.BatchNorm1d(fc[0])\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "\n",
        "        self.fc2 = nn.Linear(fc[0], fc[1])\n",
        "        self.bn7 = nn.BatchNorm1d(fc[1])\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "        self.fc3 = nn.Linear(fc[1], 2)  # Output layer for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Subtract mean\n",
        "        x = x - torch.mean(x, dim=(2, 3), keepdim=True)\n",
        "\n",
        "        # Convolutional blocks\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        # Flatten the output for the fully connected layers\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.bn6(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.relu(self.bn7(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = F.softmax(self.fc3(x), dim=1)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model and transfer it to the device\n",
        "model = DeepFocusV3()\n",
        "model.to(device)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Function to predict clarity\n",
        "def predict_clarity(dataloader, model, threshold = 0.75):\n",
        "    with torch.no_grad():  # No need to track gradients\n",
        "        for i, (images, labels) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            clear_probs = probabilities[:, 1]  # Index 1 for 'clear'\n",
        "\n",
        "            # Decide which images to exclude based on the threshold\n",
        "            for j, prob in enumerate(clear_probs):\n",
        "                if prob.item() < threshold:  # Less than 75% probability of being 'clear'\n",
        "                    exclude_indices.append(i * dataloader.batch_size + j)\n",
        "\n",
        "# Predict clarity of PCAM images\n",
        "predict_clarity(dataloader, model)\n"
      ],
      "metadata": {
        "id": "XdsqZzHZja70"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}